<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-module1/chapter-3" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">Chapter 3: Sensing and Perception for Robotics | AI Native Book</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://Habibullahdevv.github.io/ai-native-book/docs/module1/chapter-3"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Chapter 3: Sensing and Perception for Robotics | AI Native Book"><meta data-rh="true" name="description" content="For a physical AI system to interact intelligently and autonomously with its environment, it must first be able to perceive it accurately and robustly. Sensing and perception are the robot&#x27;s fundamental gateways to understanding the world around it, its own state, and the presence and behavior of other agents. This process involves two main stages: sensing, which converts physical phenomena into measurable electrical signals, and perception, which interprets these signals to construct a meaningful, actionable representation of the environment. The quality and reliability of these perceptions directly impact a robot&#x27;s ability to make informed decisions, plan movements, and execute tasks effectively."><meta data-rh="true" property="og:description" content="For a physical AI system to interact intelligently and autonomously with its environment, it must first be able to perceive it accurately and robustly. Sensing and perception are the robot&#x27;s fundamental gateways to understanding the world around it, its own state, and the presence and behavior of other agents. This process involves two main stages: sensing, which converts physical phenomena into measurable electrical signals, and perception, which interprets these signals to construct a meaningful, actionable representation of the environment. The quality and reliability of these perceptions directly impact a robot&#x27;s ability to make informed decisions, plan movements, and execute tasks effectively."><link data-rh="true" rel="icon" href="/ai-native-book/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://Habibullahdevv.github.io/ai-native-book/docs/module1/chapter-3"><link data-rh="true" rel="alternate" href="https://Habibullahdevv.github.io/ai-native-book/docs/module1/chapter-3" hreflang="en"><link data-rh="true" rel="alternate" href="https://Habibullahdevv.github.io/ai-native-book/docs/module1/chapter-3" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Chapter 3: Sensing and Perception for Robotics","item":"https://Habibullahdevv.github.io/ai-native-book/docs/module1/chapter-3"}]}</script><link rel="stylesheet" href="/ai-native-book/assets/css/styles.f068f835.css">
<script src="/ai-native-book/assets/js/runtime~main.68262f71.js" defer="defer"></script>
<script src="/ai-native-book/assets/js/main.7f69558f.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/ai-native-book/"><b class="navbar__title text--truncate">AI Native Book</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/ai-native-book/docs/intro">Book Modules</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ai-native-book/docs/intro"><span title="Introduction" class="linkLabel_WmDU">Introduction</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" href="/ai-native-book/docs/module1/chapter-1"><span title="Module 1: Foundations of Physical AI" class="categoryLinkLabel_W154">Module 1: Foundations of Physical AI</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-native-book/docs/module1/chapter-1"><span title="Chapter 1: Introduction to Physical AI and Robotics" class="linkLabel_WmDU">Chapter 1: Introduction to Physical AI and Robotics</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-native-book/docs/module1/chapter-2"><span title="Chapter 2: Robot Kinematics and Dynamics" class="linkLabel_WmDU">Chapter 2: Robot Kinematics and Dynamics</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/ai-native-book/docs/module1/chapter-3"><span title="Chapter 3: Sensing and Perception for Robotics" class="linkLabel_WmDU">Chapter 3: Sensing and Perception for Robotics</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-native-book/docs/module1/chapter-4"><span title="Chapter 4: Actuation and Control Systems" class="linkLabel_WmDU">Chapter 4: Actuation and Control Systems</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-native-book/docs/module1/chapter-5"><span title="Chapter 5: Introduction to Robot Operating System (ROS)" class="linkLabel_WmDU">Chapter 5: Introduction to Robot Operating System (ROS)</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-native-book/docs/module1"><span title="Module 1: Foundations of Physical AI" class="linkLabel_WmDU">Module 1: Foundations of Physical AI</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/ai-native-book/docs/module2/chapter1"><span title="Module 2: Human Locomotion &amp; Control" class="categoryLinkLabel_W154">Module 2: Human Locomotion &amp; Control</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/ai-native-book/docs/module3/chapter1"><span title="Module 3: Perception &amp; Intelligence" class="categoryLinkLabel_W154">Module 3: Perception &amp; Intelligence</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/ai-native-book/docs/module4/chapter1"><span title="Module 4: HRI, Safety &amp; Ethics" class="categoryLinkLabel_W154">Module 4: HRI, Safety &amp; Ethics</span></a></div></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/ai-native-book/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Module 1: Foundations of Physical AI</span></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Chapter 3: Sensing and Perception for Robotics</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Chapter 3: Sensing and Perception for Robotics</h1></header>
<p>For a physical AI system to interact intelligently and autonomously with its environment, it must first be able to perceive it accurately and robustly. Sensing and perception are the robot&#x27;s fundamental gateways to understanding the world around it, its own state, and the presence and behavior of other agents. This process involves two main stages: <strong>sensing</strong>, which converts physical phenomena into measurable electrical signals, and <strong>perception</strong>, which interprets these signals to construct a meaningful, actionable representation of the environment. The quality and reliability of these perceptions directly impact a robot&#x27;s ability to make informed decisions, plan movements, and execute tasks effectively.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="the-spectrum-of-robotic-sensors">The Spectrum of Robotic Sensors<a href="#the-spectrum-of-robotic-sensors" class="hash-link" aria-label="Direct link to The Spectrum of Robotic Sensors" title="Direct link to The Spectrum of Robotic Sensors" translate="no">​</a></h2>
<p>Robots employ a diverse array of sensors, each designed to capture specific types of information about the physical world:</p>
<p><strong>[FIGURE 3.1: Spectrum of Robotic Sensors. A diagram categorizing various robotic sensors (e.g., Cameras, LiDAR, Tactile, IMU) and their primary functions.]</strong></p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="vision-sensors">Vision Sensors<a href="#vision-sensors" class="hash-link" aria-label="Direct link to Vision Sensors" title="Direct link to Vision Sensors" translate="no">​</a></h3>
<ul>
<li class=""><strong>Monocular Cameras:</strong> Provide 2D image data, used for object detection, recognition, and tracking. While cost-effective, they lack direct depth information.</li>
<li class=""><strong>Stereo Cameras:</strong> Consist of two cameras placed side-by-side, mimicking human binocular vision. By comparing images from both cameras, they can compute depth information (disparity maps), crucial for 3D reconstruction and obstacle avoidance.</li>
<li class=""><strong>RGB-D Cameras:</strong> (e.g., Intel RealSense, Microsoft Azure Kinect) Provide both color (RGB) and depth (D) information directly. They often use technologies like structured light or Time-of-Flight (ToF) to measure distances, offering robust 3D perception in varying lighting conditions.</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="range-sensors">Range Sensors<a href="#range-sensors" class="hash-link" aria-label="Direct link to Range Sensors" title="Direct link to Range Sensors" translate="no">​</a></h3>
<ul>
<li class=""><strong>LiDAR (Light Detection and Ranging):</strong> Emits pulsed laser light and measures the time it takes for the light to return, creating highly accurate 2D or 3D point clouds of the environment. Essential for precise mapping, localization, and navigation in complex spaces.</li>
<li class=""><strong>Ultrasonic Sensors:</strong> Emit sound waves and measure the time for the echo to return. They are cost-effective for short-range obstacle detection and distance measurement, though less precise than LiDAR and susceptible to specular reflections.</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="contact-and-force-sensors">Contact and Force Sensors<a href="#contact-and-force-sensors" class="hash-link" aria-label="Direct link to Contact and Force Sensors" title="Direct link to Contact and Force Sensors" translate="no">​</a></h3>
<ul>
<li class=""><strong>Tactile Sensors (Touch Sensors):</strong> Detect physical contact and pressure, often used in robot grippers for object manipulation, surface texture recognition, and ensuring gentle interaction. Can range from simple binary contact switches to arrays capable of measuring pressure distribution.</li>
<li class=""><strong>Force/Torque Sensors:</strong> Measure the forces and torques exerted at specific points, typically at the robot&#x27;s wrist or base. Crucial for tasks requiring compliant motion, force-controlled grasping, and safe human-robot interaction where forces need to be monitored and regulated.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="proprioceptive-sensors-internal-awareness">Proprioceptive Sensors: Internal Awareness<a href="#proprioceptive-sensors-internal-awareness" class="hash-link" aria-label="Direct link to Proprioceptive Sensors: Internal Awareness" title="Direct link to Proprioceptive Sensors: Internal Awareness" translate="no">​</a></h2>
<p>Beyond sensing the external environment, robots need to understand their own body state. <strong>Proprioceptive sensors</strong> provide internal feedback:</p>
<ul>
<li class=""><strong>Encoders:</strong> Measure the angular position or displacement of robot joints. Crucial for precise control of motor movements and determining the robot&#x27;s kinematic configuration.</li>
<li class=""><strong>IMUs (Inertial Measurement Units):</strong> Combine accelerometers (measuring linear acceleration) and gyroscopes (measuring angular velocity) to estimate the robot&#x27;s orientation and motion in space. Advanced IMUs may also include magnetometers for heading estimation.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="perception-algorithms-interpreting-sensor-data">Perception Algorithms: Interpreting Sensor Data<a href="#perception-algorithms-interpreting-sensor-data" class="hash-link" aria-label="Direct link to Perception Algorithms: Interpreting Sensor Data" title="Direct link to Perception Algorithms: Interpreting Sensor Data" translate="no">​</a></h2>
<p>Perception algorithms transform raw sensor data into meaningful information that higher-level cognitive processes can use. Key techniques include:</p>
<ul>
<li class=""><strong>Image Processing:</strong> Fundamental operations on visual data, such as filtering, edge detection, segmentation, and feature extraction (e.g., SIFT, SURF) to prepare images for analysis.</li>
<li class=""><strong>Object Detection and Tracking:</strong> Identifying and localizing specific objects within sensor data (e.g., using deep learning models like YOLO, Faster R-CNN) and subsequently following their movement over time. Crucial for manipulation and interaction.</li>
<li class=""><strong>Simultaneous Localization and Mapping (SLAM):</strong> A concurrent process where a robot builds a map of an unknown environment while simultaneously estimating its own position within that map. This is vital for autonomous navigation in novel environments.</li>
<li class=""><strong>State Estimation:</strong> Using probabilistic filters (e.g., <strong>Kalman Filter</strong>, <strong>Extended Kalman Filter (EKF)</strong>, <strong>Particle Filter</strong>) to combine noisy sensor measurements with a robot&#x27;s motion model to produce a more accurate estimate of its true state (position, velocity, orientation) over time.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="sensor-fusion-a-holistic-view">Sensor Fusion: A Holistic View<a href="#sensor-fusion-a-holistic-view" class="hash-link" aria-label="Direct link to Sensor Fusion: A Holistic View" title="Direct link to Sensor Fusion: A Holistic View" translate="no">​</a></h2>
<p>Sensor fusion is the process of combining data from multiple sensors to obtain a more complete, accurate, and reliable understanding of the environment and the robot&#x27;s state than could be achieved by using individual sensors alone. This approach addresses the limitations of individual sensors (e.g., a camera needs sufficient light, LiDAR can be fooled by reflective surfaces, IMUs drift over time).</p>
<p>Key benefits of sensor fusion:</p>
<ul>
<li class=""><strong>Increased Accuracy:</strong> Combining complementary data often reduces overall noise and error.</li>
<li class=""><strong>Enhanced Robustness:</strong> If one sensor fails or provides ambiguous data, other sensors can compensate.</li>
<li class=""><strong>Broader Coverage:</strong> Different sensors capture different aspects of the environment, providing a richer perception.</li>
</ul>
<p>Common sensor fusion techniques include various forms of Kalman filters (e.g., fusing IMU data with GPS or vision for improved navigation) and probabilistic grid mapping for combining range sensor data. The ability to seamlessly integrate and interpret data from a heterogeneous sensor suite is a hallmark of robust physical AI systems operating in complex, dynamic, and often unpredictable real-world environments. The development of advanced perception systems, often leveraging deep learning models for feature extraction and semantic understanding, remains an ongoing challenge and a cornerstone of effective physical AI.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"><a href="https://github.com/Habibullahdevv/ai-native-book/tree/main/docs/module1/chapter-3.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/ai-native-book/docs/module1/chapter-2"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Chapter 2: Robot Kinematics and Dynamics</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/ai-native-book/docs/module1/chapter-4"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Chapter 4: Actuation and Control Systems</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#the-spectrum-of-robotic-sensors" class="table-of-contents__link toc-highlight">The Spectrum of Robotic Sensors</a><ul><li><a href="#vision-sensors" class="table-of-contents__link toc-highlight">Vision Sensors</a></li><li><a href="#range-sensors" class="table-of-contents__link toc-highlight">Range Sensors</a></li><li><a href="#contact-and-force-sensors" class="table-of-contents__link toc-highlight">Contact and Force Sensors</a></li></ul></li><li><a href="#proprioceptive-sensors-internal-awareness" class="table-of-contents__link toc-highlight">Proprioceptive Sensors: Internal Awareness</a></li><li><a href="#perception-algorithms-interpreting-sensor-data" class="table-of-contents__link toc-highlight">Perception Algorithms: Interpreting Sensor Data</a></li><li><a href="#sensor-fusion-a-holistic-view" class="table-of-contents__link toc-highlight">Sensor Fusion: A Holistic View</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/ai-native-book/docs/module1">Module 1</a></li><li class="footer__item"><a class="footer__link-item" href="/ai-native-book/docs/module2">Module 2</a></li><li class="footer__item"><a class="footer__link-item" href="/ai-native-book/docs/module3">Module 3</a></li><li class="footer__item"><a class="footer__link-item" href="/ai-native-book/docs/module4">Module 4</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://twitter.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/Habibullahdevv/ai-native-book" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 My Book, Inc. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>