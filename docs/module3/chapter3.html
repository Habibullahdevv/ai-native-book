<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-module3/chapter3" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">SLAM &amp; Spatial Understanding | AI Native Book</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://Habibullahdevv.github.io/ai-native-book/docs/module3/chapter3"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="SLAM &amp; Spatial Understanding | AI Native Book"><meta data-rh="true" name="description" content="Simultaneous Localization and Mapping (SLAM) is a fundamental problem in robotics, enabling a robot to build a map of an unknown environment while simultaneously tracking its own pose (position and orientation) within that map. For AI-powered robots, especially humanoids, robust SLAM and spatial understanding are critical for autonomous navigation, interaction, and task execution. This chapter explores various SLAM techniques and how they contribute to a robot&#x27;s spatial awareness, with a focus on NVIDIA Isaac ROS integration."><meta data-rh="true" property="og:description" content="Simultaneous Localization and Mapping (SLAM) is a fundamental problem in robotics, enabling a robot to build a map of an unknown environment while simultaneously tracking its own pose (position and orientation) within that map. For AI-powered robots, especially humanoids, robust SLAM and spatial understanding are critical for autonomous navigation, interaction, and task execution. This chapter explores various SLAM techniques and how they contribute to a robot&#x27;s spatial awareness, with a focus on NVIDIA Isaac ROS integration."><link data-rh="true" rel="icon" href="/ai-native-book/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://Habibullahdevv.github.io/ai-native-book/docs/module3/chapter3"><link data-rh="true" rel="alternate" href="https://Habibullahdevv.github.io/ai-native-book/docs/module3/chapter3" hreflang="en"><link data-rh="true" rel="alternate" href="https://Habibullahdevv.github.io/ai-native-book/docs/module3/chapter3" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"SLAM & Spatial Understanding","item":"https://Habibullahdevv.github.io/ai-native-book/docs/module3/chapter3"}]}</script><link rel="stylesheet" href="/ai-native-book/assets/css/styles.834ea21a.css">
<script src="/ai-native-book/assets/js/runtime~main.68262f71.js" defer="defer"></script>
<script src="/ai-native-book/assets/js/main.68fcc8b7.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/ai-native-book/"><b class="navbar__title text--truncate">AI Native Book</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/ai-native-book/docs/intro">Book Modules</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ai-native-book/docs/intro"><span title="Introduction" class="linkLabel_WmDU">Introduction</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/ai-native-book/docs/module1/chapter-1"><span title="Module 1: Foundations of Physical AI" class="categoryLinkLabel_W154">Module 1: Foundations of Physical AI</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/ai-native-book/docs/module2/chapter1"><span title="Module 2: Human Locomotion &amp; Control" class="categoryLinkLabel_W154">Module 2: Human Locomotion &amp; Control</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" href="/ai-native-book/docs/module3/chapter1"><span title="Module 3: Perception &amp; Intelligence" class="categoryLinkLabel_W154">Module 3: Perception &amp; Intelligence</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-native-book/docs/module3/chapter1"><span title="Computer Vision in Robotics" class="linkLabel_WmDU">Computer Vision in Robotics</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-native-book/docs/module3/chapter2"><span title="Multimodal Sensing Fusion" class="linkLabel_WmDU">Multimodal Sensing Fusion</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/ai-native-book/docs/module3/chapter3"><span title="SLAM &amp; Spatial Understanding" class="linkLabel_WmDU">SLAM &amp; Spatial Understanding</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-native-book/docs/module3/chapter4"><span title="Sensor Calibration" class="linkLabel_WmDU">Sensor Calibration</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-native-book/docs/module3/chapter5"><span title="AI Reasoning in Robotics Systems" class="linkLabel_WmDU">AI Reasoning in Robotics Systems</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-native-book/docs/module3"><span title="Module 3: Perception and Intelligence in Physical AI Systems" class="linkLabel_WmDU">Module 3: Perception and Intelligence in Physical AI Systems</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/ai-native-book/docs/module4/chapter1"><span title="Module 4: HRI, Safety &amp; Ethics" class="categoryLinkLabel_W154">Module 4: HRI, Safety &amp; Ethics</span></a></div></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/ai-native-book/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Module 3: Perception &amp; Intelligence</span></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">SLAM &amp; Spatial Understanding</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>SLAM &amp; Spatial Understanding</h1></header>
<p>Simultaneous Localization and Mapping (SLAM) is a fundamental problem in robotics, enabling a robot to build a map of an unknown environment while simultaneously tracking its own pose (position and orientation) within that map. For AI-powered robots, especially humanoids, robust SLAM and spatial understanding are critical for autonomous navigation, interaction, and task execution. This chapter explores various SLAM techniques and how they contribute to a robot&#x27;s spatial awareness, with a focus on NVIDIA Isaac ROS integration.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="the-slam-problem">The SLAM Problem<a href="#the-slam-problem" class="hash-link" aria-label="Direct link to The SLAM Problem" title="Direct link to The SLAM Problem" translate="no">​</a></h2>
<p>The core challenge of SLAM is the chicken-and-egg problem: an accurate map is needed for precise localization, but accurate localization is required to build a consistent map. SLAM systems continuously refine both the map and the robot&#x27;s pose through an iterative process, typically involving:</p>
<ul>
<li class=""><strong>Odometry/Motion Model</strong>: Estimating the robot&#x27;s movement between consecutive sensor readings (e.g., visual odometry from cameras, inertial odometry from IMUs).</li>
<li class=""><strong>Sensor Model</strong>: Predicting sensor readings given the robot&#x27;s pose and the map, and comparing with actual readings to update the map and pose.</li>
<li class=""><strong>Data Association</strong>: Matching current sensor readings to features in the existing map.</li>
<li class=""><strong>Loop Closure</strong>: Recognizing previously visited locations to correct accumulated errors (drift) in the map and pose, creating a globally consistent map.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="types-of-slam">Types of SLAM<a href="#types-of-slam" class="hash-link" aria-label="Direct link to Types of SLAM" title="Direct link to Types of SLAM" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="1-visual-slam-vslam">1. Visual SLAM (VSLAM)<a href="#1-visual-slam-vslam" class="hash-link" aria-label="Direct link to 1. Visual SLAM (VSLAM)" title="Direct link to 1. Visual SLAM (VSLAM)" translate="no">​</a></h3>
<p>Utilizes camera images as the primary sensor input. VSLAM methods can be:</p>
<ul>
<li class=""><strong>Feature-based</strong>: Extracts distinct features (e.g., corners, SIFT, ORB) from images and tracks them across frames.</li>
<li class=""><strong>Direct/Semi-Direct</strong>: Directly uses pixel intensities to estimate motion, often more robust in texture-less environments.</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="2-lidar-slam">2. LiDAR SLAM<a href="#2-lidar-slam" class="hash-link" aria-label="Direct link to 2. LiDAR SLAM" title="Direct link to 2. LiDAR SLAM" translate="no">​</a></h3>
<p>Employs LiDAR sensor data to build highly accurate 3D point cloud maps. LiDAR SLAM is less affected by lighting changes and provides direct depth measurements, making it suitable for larger, outdoor environments.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="3-visual-inertial-slam-v-i-slam">3. Visual-Inertial SLAM (V-I SLAM)<a href="#3-visual-inertial-slam-v-i-slam" class="hash-link" aria-label="Direct link to 3. Visual-Inertial SLAM (V-I SLAM)" title="Direct link to 3. Visual-Inertial SLAM (V-I SLAM)" translate="no">​</a></h3>
<p>Combines visual information from cameras with inertial data from IMUs. IMU data helps to provide high-frequency motion estimates, which can mitigate the effects of rapid movements and provide scale information, complementing the visual data which can be prone to scale drift.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="spatial-understanding-and-semantic-slam">Spatial Understanding and Semantic SLAM<a href="#spatial-understanding-and-semantic-slam" class="hash-link" aria-label="Direct link to Spatial Understanding and Semantic SLAM" title="Direct link to Spatial Understanding and Semantic SLAM" translate="no">​</a></h2>
<p>Beyond just creating geometric maps, advanced robots require <em>spatial understanding</em> – the ability to interpret the meaning and function of objects and regions in the environment. This leads to <strong>Semantic SLAM</strong>, where the map includes not just geometric information but also semantic labels (e.g., &quot;this is a chair,&quot; &quot;this is a door&quot;). This enables robots to:</p>
<ul>
<li class=""><strong>Navigate intelligently</strong>: Avoid specific objects, use doorways, or find particular types of locations.</li>
<li class=""><strong>Interact contextually</strong>: Understand that a &quot;cup&quot; can be grasped, or a &quot;table&quot; can be set.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="nvidia-isaac-ros-for-slam">NVIDIA Isaac ROS for SLAM<a href="#nvidia-isaac-ros-for-slam" class="hash-link" aria-label="Direct link to NVIDIA Isaac ROS for SLAM" title="Direct link to NVIDIA Isaac ROS for SLAM" translate="no">​</a></h2>
<p>NVIDIA Isaac ROS provides highly optimized, GPU-accelerated packages for various SLAM tasks. These include:</p>
<ul>
<li class=""><strong>Visual SLAM modules</strong>: Leveraging GPU power for real-time feature extraction, matching, and optimization.</li>
<li class=""><strong>Graph-based SLAM solvers</strong>: Efficiently optimizing the robot&#x27;s trajectory and map by representing them as a graph.</li>
<li class=""><strong>Integration with Nav2</strong>: Providing accurate pose estimates and maps to the ROS 2 navigation stack for autonomous locomotion.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion" translate="no">​</a></h2>
<p>SLAM and spatial understanding are foundational pillars for autonomous robots. By continuously building and refining maps while simultaneously localizing themselves, robots gain the essential environmental awareness needed for navigation, planning, and intelligent interaction. The integration of advanced computational capabilities, particularly through platforms like NVIDIA Isaac ROS, allows for the real-time, robust SLAM performance required for complex humanoid robotics applications.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"><a href="https://github.com/Habibullahdevv/ai-native-book/tree/main/docs/module3/chapter3.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/ai-native-book/docs/module3/chapter2"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Multimodal Sensing Fusion</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/ai-native-book/docs/module3/chapter4"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Sensor Calibration</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#the-slam-problem" class="table-of-contents__link toc-highlight">The SLAM Problem</a></li><li><a href="#types-of-slam" class="table-of-contents__link toc-highlight">Types of SLAM</a><ul><li><a href="#1-visual-slam-vslam" class="table-of-contents__link toc-highlight">1. Visual SLAM (VSLAM)</a></li><li><a href="#2-lidar-slam" class="table-of-contents__link toc-highlight">2. LiDAR SLAM</a></li><li><a href="#3-visual-inertial-slam-v-i-slam" class="table-of-contents__link toc-highlight">3. Visual-Inertial SLAM (V-I SLAM)</a></li></ul></li><li><a href="#spatial-understanding-and-semantic-slam" class="table-of-contents__link toc-highlight">Spatial Understanding and Semantic SLAM</a></li><li><a href="#nvidia-isaac-ros-for-slam" class="table-of-contents__link toc-highlight">NVIDIA Isaac ROS for SLAM</a></li><li><a href="#conclusion" class="table-of-contents__link toc-highlight">Conclusion</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/ai-native-book/docs/module1">Module 1</a></li><li class="footer__item"><a class="footer__link-item" href="/ai-native-book/docs/module2">Module 2</a></li><li class="footer__item"><a class="footer__link-item" href="/ai-native-book/docs/module3">Module 3</a></li><li class="footer__item"><a class="footer__link-item" href="/ai-native-book/docs/module4">Module 4</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://twitter.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/Habibullahdevv/ai-native-book" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 My Book, Inc. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>